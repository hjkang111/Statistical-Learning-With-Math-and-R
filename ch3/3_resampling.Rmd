---
title: "Chapter 3: Resampling"
subtitle: "CV Formula for Linear Regression"
author: "JuHyun Kang"
date: "March 2, 2025"
output:
  beamer_presentation:
    latex_engine: xelatex
    theme: metropolis
  slidy_presentation: default
fonttheme: serif
fontsize: 8pt
institute: The Three Sisters of Newton \newline School of Mathematics, Statistics and Data Science \newline Sungshin Women's University
header-includes: \input{header_includes.tex}
---

# Distribution of the RSS Values

## Hat Matrix
- Hat matrix defined by 


  $\hat{y} = X\hat{\beta} = X(X^TX)^{-1}{X^T}y = Hy$
$$H \mathrel{:=} X(X^TX)^{-1}{X^T}$$


- Some properties
\begin{enumerate}
\vspace{0.15cm}
\item $H^2 = X(X^{T} X)^{-1}X^{T} \cdot X(X^{T} X)^{-1}X^{T} = X(X^{T} X)^{-1}(X^{T}X)(X^{T} X)^{-1}X^{T}$ \\ $= X(X^{T} X)^{-1}X^{T} = H$
\vspace{0.15cm}
\item $(I -H)^2 = I-2H+H^{2} = I-2H+H = I-H$
\vspace{0.15cm}
\item $HX = X(X^{T} X)^{-1}X^{T} \cdot X = X$
\vspace{0.15cm}
\item $H^{T} = \{ X(X^{T} X)^{-1}X^{T} \}^{T} = X(X^{T} X)^{-1}X^{T} = H$
\end{enumerate}

## Residual Sum of Squares

- RSS defined
$$
\text{RSS} \mathrel{:=} ||y-\hat{y}||^2
$$

- Using hat matrix
\begin{align*}
y - \hat{y} &= y - Hy = (I-H)y = (I-H)(X\beta+\varepsilon) \\
            &= (X -HX)\beta + (I - H)\varepsilon = (I - H)\varepsilon
\end{align*}

$$
||y - \hat{y}||^2 = \{(I - H)\varepsilon\}^T (I - H)\varepsilon = \varepsilon^T(I-H)^2\varepsilon = \varepsilon^T(I-H)\varepsilon
$$

## Eigenvalues $H$ and $I-H$

- They are only zeros and ones
\vt
- Dimensions of the eigenspaces of $H$ and $I-H$ are both $p + 1$ \
\vspace{0.1cm}
\textbf{Proof} using rank$(X) = p+1$ 
\begin{align*}
\text{rank}(H) &\le \text{min}\{\text{rank}(X(X^TX)^{-1}), \text{rank}(X)\} \le \text{rank}(X) = p + 1 \\
\text{rank}(H) &\ge \text{rank}(HX) = \text{rank}(X) = p + 1
\end{align*}


## Eigenvalues of $H$ and Null space of ($I-H$)
- Proof by contrapositive
\begin{align*}
Hx = x \Rightarrow (I - H)x = 0 \\
(I - H)x = 0 \Rightarrow Hx = x
\end{align*}
- Dimensions of the eigenspaces of $H$ is $p + 1$ \
\vt

- Dimensions of the null space of $I-H$ is $N - (p+1)$

$$
P (I - H) P^T = \operatorname{diag}(\underbrace{1, \dots, 1}_{N-p-1}, \underbrace{0, \dots, 0}_{p+1})
$$

## RSS Matrix
-  We define $v = P\varepsilon \in \mathbb{R}^N$, then from $\varepsilon = P^Tv$
\begin{align*}
\text{RSS} 
&= \varepsilon^T(I - H)\varepsilon = (P^Tv)^T(I-H)P^Tv = v^TP(I-H)P^Tv \\
&= [v_1, \cdots, v_{N-p-1}, v_{N-p}, \cdots, v_n] 
\begin{bmatrix}
1      & 0      &\cdots  &  \cdots & \cdots & 0 \\
0      & \ddots & 0      &  \cdots & \cdots & \vdots \\
\vdots & 0      & 1      &  \cdots & \cdots & 0 \\
0      & 0      & 0      &  \cdots & \cdots & \cdots \\
\vdots & \vdots & \vdots &  \vdots & \ddots & \vdots \\
0      & \cdots & 0      &  \cdots & \cdots & 0
\end{bmatrix}
\begin{bmatrix}
v_1 \\
\vdots \\
v_{N-p-1} \\
v_{N-p} \\
\vdots \\
v_N
\end{bmatrix} 
&= \sum^{N-p-1}_{i = 1}v_i^2
\end{align*}

## Distribution of RSS
- Let $w \in \mathbb{R}^{N-p-1}$
\vt
- Average 

  $E[v] = E[P\varepsilon] = 0$

  $E[w] = 0$
\vt

- Covariance 

  $E[vv^t] = E[P\varepsilon(P\varepsilon)^T] = PE[\varepsilon\varepsilon^t]P=P\sigma^2IP^T = \sigma^2I$
  
  $E[ww^T] = \sigma^2I$
\vt
- $w$~$N(0,\sigma^2I)$, $\frac{w^Tw}{\sigma^2}$ is the sum of squares of $N-p-1$ independent standard normal variables

- We have RSS
$$
\frac{RSS}{\sigma^2} \sim \chi^2_{N-p-1}
$$

# Hypothesis Testing for $\hat{\beta}_j \ne 0$

## Test Statistic $T$
- T distribution with $N - p -1$ degrees of freedom
\vt
- We decide that hypothesis $\beta_j = 0$ should be rejected.
\vt
- $U \sim N(0,1),\ V \sim \chi^2_m$, 

$$
T \triangleq U / \sqrt{V/m}
$$

```{r echo=FALSE, fig.height=5, fig.width=9, message=FALSE}
curve(dnorm(x), -10, 10, ann = FALSE, ylim = c(0, 0.5), lwd = 5)
for(i in 1:10)curve(dt(x, df= i), -10, 10, col = i, add = TRUE, ann = FALSE)
legend("topright", legend = 1:10, lty = 1, col = 1:10)
```

## Significance Level
- $\alpha = 0.01,\ 0.05$
\vt
- Null hypothesis $\beta_j = 0$

```{r fig.height=5, fig.width=9, echo=FALSE}
# 기본 정규분포 그래프
curve(dnorm(x), -6, 6, ann = FALSE, ylim = c(0, 0.6), lwd = 3)

# 유의수준(alpha) 설정
alpha <- 0.05  # 95% 신뢰구간 (5% 유의수준)
z_crit <- qnorm(1 - alpha/2)  # 양측 검정의 임계값

# 기각영역 채우기 (좌측, 우측)
x_reject_left <- seq(-6, -z_crit, length=100)
x_reject_right <- seq(z_crit, 6, length=100)

polygon(c(x_reject_left, rev(x_reject_left)), 
        c(dnorm(x_reject_left), rep(0, length(x_reject_left))), 
        col="red", border=NA)

polygon(c(x_reject_right, rev(x_reject_right)), 
        c(dnorm(x_reject_right), rep(0, length(x_reject_right))), 
        col="red", border=NA)

# 채택영역 채우기 (가운데 부분)
x_accept <- seq(-z_crit, z_crit, length=100)
polygon(c(x_accept, rev(x_accept)), 
        c(dnorm(x_accept), rep(0, length(x_accept))), 
        col="blue", border=NA)

# 텍스트 추가
text(0, 0.25, expression(1 - alpha), col = "black", cex=1.5)
text(-4, 0.05, expression(alpha/2), col = "black", cex=1.2)
text(4, 0.05, expression(alpha/2), col = "black", cex=1.2)
text(-4, 0.1, "REJECT", col="red", cex=1.2)
text(4, 0.1, "REJECT", col="red", cex=1.2)
text(0, 0.4, "ACCEPT", col="black", cex=1.5)

# 수직선 추가 (임계값)
abline(v = c(-z_crit, z_crit), col="red", lwd=2)

# 범례 추가
legend("topright", legend=c("Standard Normal", "Reject Region", "Accept Region"),
       col=c("black", "red", "blue"), lty=1, fill=c(NA, "red", "blue"), border=NA)

```

## Example 23
- For $p=1$, since
$$
X^TX = \begin{bmatrix} 1 & \cdots   & 1 \\
                       x_1 & \cdots & x_N
       \end{bmatrix}
       \begin{bmatrix}
       1 & x_1 \\
       \vdots & \vdots \\
       1 & x_N
       \end{bmatrix}
       = N \begin{bmatrix}
            1 & \bar{x} \\
            \bar{x} & \frac{1}{N}\sum_{i = 1}^{N}x_i^2
           \end{bmatrix}
$$
- The inverse is
$$
(X^TX)^{-1}  = \frac{1}{\sum_{i=1}^{N}(x_i - \bar{x})^2}
                         \begin{bmatrix}
                            \frac{1}{N}\sum_{i = 1}^{N}x_i^2       & -\bar{x} \\
                            -\bar{x}                               & 1
                         \end{bmatrix}
$$

- Which means that
$$
B_0 = \frac{\frac{1}{N}\sum_{i = 1}^{N}x_i^2}{\sum_{i=1}^{N}(x_i - \bar{x})^2} \ \ \ \text{and} \ \ \ B_1 = \frac{1}{\sum_{i=1}^{N}(x_i - \bar{x})^2}
$$

## Example 23 (contd.)
- For $B = (X^TX)^{-1}, B\sigma^2$ is covariance matrix of $\hat{\beta}$

- $B_j\sigma^2$ is the variance of $\hat{\beta}_j$

- Because $\bar{x}$ is positive, the correlation between $\hat{\beta}_0$ and $\hat{\beta}_1$ is negative
$$
t = \frac{\hat{\beta}_j - \beta_j}{\text{SE}(\hat{\beta}_j)} \sim t_{N-p-1}
$$

## Statistical Independence in Regression
- It remains to be shown that $U$ and $V$ are independent
$$
U \triangleq \frac{\hat{\beta}_j-\beta_j}{\sqrt{B_j}\sigma} \sim N(0,1) \ \ \ \text{and} \ \ \ V \triangleq \chi^2_{N-p-1}
$$
- Sufficient to show that $y - \hat{y}$ and $\hat{\beta} - \beta$ are independent
$$
(\hat{\beta} - \beta)(y-\hat{y})^T = (X^TX)^{-1}X^T\varepsilon\varepsilon^T(I-H)
$$

- From $E\varepsilon\varepsilon^T =\sigma^2I$ and $HX = X$,
$$
E(\hat{\beta} -\beta)(y-\hat{y})^T = 0
$$


## Q & A

\begin{center}
  {\bf {\Huge Q \& A}}
\end{center}

## 

\begin{center}
  {\bf {\Huge Thank you :)}}
\end{center}

